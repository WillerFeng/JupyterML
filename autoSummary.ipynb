{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from collections import defaultdict, Counter\n",
    "from collections import namedtuple\n",
    "from porterStemmer import PorterStemmer\n",
    "from functools import reduce\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn import cluster as skc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "__author__ = 'willer_sjf'\n",
    "\n",
    "#             <--------------------------------->\n",
    "#             <                                 >\n",
    "#             <       SWIFT PHILOSOPHY !!       >\n",
    "#             <                                 >\n",
    "#             <--------------------------------->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtilityFunction:\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    @classmethod\n",
    "    def stemFunction(cls, sentences):\n",
    "        newSentences = []\n",
    "        for sentence in sentences:\n",
    "            output = ''\n",
    "            word = ''\n",
    "            for c in sentence:\n",
    "                if c.isalpha():\n",
    "                    word += c.lower()\n",
    "                else:\n",
    "                    if word:\n",
    "                        output += cls.stemmer.stem(word, 0, len(word)-1)\n",
    "                        word = ''\n",
    "                    output += c.lower()\n",
    "            if len(output) >= 5:\n",
    "                newSentences.append(output)\n",
    "        return newSentences\n",
    "    \n",
    "    @staticmethod\n",
    "    def ROUGE_N(result, label, total, n=1):\n",
    "        resultDictionary = Counter(result.split())\n",
    "        labelDictionary  = Counter(label.split())\n",
    "        totalDictionary  = Counter(total.split())\n",
    "        confusionMatrix = [[0.0, 0.0], \n",
    "                           [0.0, 0.0]]\n",
    "        for key, value in labelDictionary.items():\n",
    "            confusionMatrix[0][0] += min(value , resultDictionary[key])\n",
    "            confusionMatrix[1][1] += max(value - resultDictionary[key], 0)\n",
    "            \n",
    "        for key, value in resultDictionary.items():\n",
    "            confusionMatrix[0][1] += max(value - labelDictionary[key],0)\n",
    "            \n",
    "        labelDictionary += resultDictionary\n",
    "        for key, value in totalDictionary.items():\n",
    "            confusionMatrix[1][0] += value - labelDictionary[key]\n",
    "        \n",
    "        return confusionMatrix\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosineSimilarity(vectorA, vectorB):\n",
    "        return np.dot(vectorA, vectorB.transpose()) / (np.sqrt(np.sum(vectorA ** 2)) * np.sqrt(np.sum(vectorA ** 2)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def l2Distance(vectorA, vectorB):\n",
    "        return np.sqrt(np.sum((vectorA - vectorB) ** 2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def cluster(sentences, clusterSize):\n",
    "        X   = list(zip(*sentences))[1]\n",
    "        centroid, label, _ = skc.k_means(X , clusterSize)\n",
    "        res = []\n",
    "        for c in centroid:\n",
    "            res.append([c, []])\n",
    "        for i,l in enumerate(label):\n",
    "            res[l][1].append(sentences[i])\n",
    "        return res\n",
    "        \n",
    "    @staticmethod\n",
    "    def ROUGE_L():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoSummary:\n",
    "    \n",
    "    def __init__(self, stopwordsFile, stemFunction):\n",
    "        \n",
    "        self.stemFunction = stemFunction\n",
    "        self.stopwords    = set()\n",
    "        self.words        = None\n",
    "        self.delta        = 0.00001\n",
    "        self.epsilon      = 0.0001\n",
    "        self.threshold    = 0.2\n",
    "        self.d            = 0.15\n",
    "        self.size         = 665\n",
    "        self.simThreshold = 0.7\n",
    "        with open(stopwordsFile) as f:\n",
    "            for line in f:\n",
    "                self.stopwords |= set(line.split())\n",
    "    \n",
    "    def calculateSimilarMatrix(self, sentences, docs=None, getEmbedding=False) -> [[float]]:\n",
    "        \"\"\"\n",
    "        sentences: [[str]]\n",
    "        Calculate sentences self-similar matrix or doc-sentence similar matrix\n",
    "        also embedding vector\n",
    "        \"\"\"\n",
    "        if docs:\n",
    "            tfSen  = pd.DataFrame(np.zeros((len(sentences),len(self.words))), columns=list(self.words))\n",
    "            tfDoc  = pd.DataFrame(np.zeros((len(docs)     ,len(self.words))), columns=list(self.words))\n",
    "            isfSen = pd.DataFrame(np.ones((1              ,len(self.words))), columns=list(self.words))\n",
    "            isfDoc = pd.DataFrame(np.ones((1              ,len(self.words))), columns=list(self.words))\n",
    "\n",
    "            for i,s in enumerate(sentences):\n",
    "                dic = Counter(s.split())\n",
    "                for k,v in dic.items():\n",
    "                    if k in self.words:\n",
    "                        tfSen .iloc[i][k] += v / len(s)\n",
    "                        isfSen.iloc[0][k] += 1\n",
    "                        \n",
    "            for i,d in enumerate(docs):\n",
    "                dic = Counter(d.split())\n",
    "                for k,v in dic.items():\n",
    "                    if k in self.words:\n",
    "                        tfDoc .iloc[i][k] += v / len(d)\n",
    "                        isfDoc.iloc[0][k] += 1\n",
    "                \n",
    "            isfSen = np.log(len(sentences) / isfSen) \n",
    "            isfDoc = np.log(len(docs)      / isfDoc)\n",
    "            for i in range(len(sentences)):\n",
    "                tfSen.iloc[i] = tfSen.iloc[i].mul(isfSen.iloc[0])\n",
    "            for i in range(len(docs)):\n",
    "                tfDoc.iloc[i] = tfDoc.iloc[i].mul(isfDoc.iloc[0])\n",
    "            \n",
    "            tfSen         = np.array(tfSen)\n",
    "            tfDoc         = np.array(tfDoc)\n",
    "            similarMatrix = np.dot(tfSen, tfDoc.transpose())\n",
    "            innerSen      = np.sum(np.multiply(tfSen, tfSen), axis=1, keepdims=True)\n",
    "            innerDoc      = np.sum(np.multiply(tfDoc, tfDoc), axis=1, keepdims=True)\n",
    "            product       = np.sqrt(np.dot(innerSen, innerDoc.transpose()))\n",
    "            similarMatrix = similarMatrix / product\n",
    "            \n",
    "        else:\n",
    "            tf  = pd.DataFrame(np.zeros((len(sentences),len(self.words))), columns=list(self.words))\n",
    "            isf = pd.DataFrame(np.ones((1,              len(self.words))), columns=list(self.words))\n",
    "\n",
    "            for i,s in enumerate(sentences):\n",
    "                dic = Counter(s.split())\n",
    "                for k,v in dic.items():\n",
    "                    if k in self.words:\n",
    "                        tf .iloc[i][k] += v / len(s)\n",
    "                        isf.iloc[0][k] += 1\n",
    "                        \n",
    "            isf = np.log(len(sentences) / isf) \n",
    "            for i in range(len(sentences)):\n",
    "                tf.iloc[i] = tf.iloc[i].mul(isf.iloc[0])\n",
    "            tf_isf_Matrix  = np.array(tf)\n",
    "            if getEmbedding:\n",
    "                return tf_isf_Matrix\n",
    "            innerMatrix    = np.sum(np.multiply(tf_isf_Matrix, tf_isf_Matrix), axis=1, keepdims=True)\n",
    "            innerMatrix    = np.sqrt(np.dot(innerMatrix, innerMatrix.transpose()))\n",
    "            similarMatrix  = np.dot(tf_isf_Matrix, tf_isf_Matrix.transpose())\n",
    "            similarMatrix /= innerMatrix\n",
    "        \n",
    "        gc.collect()\n",
    "        return similarMatrix\n",
    "    \n",
    "    \n",
    "    def sentencesSort(self, docList, method) -> [str]:\n",
    "        \"\"\"\n",
    "        docList = [[str]]\n",
    "        method in {'lexrank' , 'dochits', 'cluster', 'cosine', 'none'}\n",
    "        \"\"\"\n",
    "        sentences = reduce(lambda x,y : x+y, docList)\n",
    "        \n",
    "        \n",
    "        if method == 'lexrank':\n",
    "\n",
    "            similarMatrix = self.calculateSimilarMatrix(sentences)\n",
    "            degree        = np.zeros(len(sentences))\n",
    "            for i in range(len(sentences)):\n",
    "                for j in range(len(sentences)):\n",
    "                    if similarMatrix[i][j] > self.threshold:\n",
    "                        similarMatrix[i][j] = 1\n",
    "                        degree[i]          += 1\n",
    "                    else:\n",
    "                        similarMatrix[i][j] = 0\n",
    "                \n",
    "            for i in range(len(sentences)):\n",
    "                for j in range(len(sentences)):\n",
    "                    similarMatrix[i][j]    /= degree[i]\n",
    "                \n",
    "            U = np.ones((len(sentences), len(sentences))) / len(sentences)\n",
    "            similarMatrix = self.d * U + (1 - self.d) * similarMatrix\n",
    "            last_p = p = np.ones(len(sentences)) / len(sentences)\n",
    "            while True:\n",
    "                p    = np.dot(similarMatrix.transpose(), p)\n",
    "                loss = np.sum(np.abs(p - last_p))\n",
    "                if loss < self.epsilon:\n",
    "                    break\n",
    "                last_p = p     \n",
    "            sentences = list(zip(sentences, p))\n",
    "            sentences.sort(key = lambda x: x[1], reverse=True)\n",
    "            sentences = list(zip(*sentences))[0]\n",
    "        \n",
    "        elif method == 'dochits':\n",
    "    \n",
    "            docs   = [reduce(lambda x,y : x+y, doc) for doc in docList]\n",
    "            L      = self.calculateSimilarMatrix(sentences, docs)\n",
    "            last_A = A = np.ones((len(sentences), 1))\n",
    "            last_H = H = np.ones((len(docs)     , 1))\n",
    "            while True:\n",
    "                A  = np.dot(L            , last_H)\n",
    "                H  = np.dot(L.transpose(), last_A)\n",
    "                A /= np.linalg.norm(A)\n",
    "                H /= np.linalg.norm(H)\n",
    "                loss_A = np.sum(last_A - A)\n",
    "                loss_H = np.sum(last_H - H)\n",
    "                if max(loss_A, loss_H) < self.delta:\n",
    "                    break\n",
    "                last_A = A\n",
    "                last_H = H\n",
    "            sentences = list(zip(sentences, A))\n",
    "            sentences.sort(key = lambda x: x[1], reverse=True)\n",
    "            sentences = list(zip(*sentences))[0]\n",
    "            \n",
    "        elif method == 'cluster':\n",
    "            docSentence    = [reduce(lambda x,y : x+y, doc) for doc in docList]\n",
    "            totalSentence  = reduce(lambda x,y : x+y, docSentence)\n",
    "            countCluster   = math.floor(np.sqrt(len(sentences)))\n",
    "            embedding      = self.calculateSimilarMatrix(sentences, getEmbedding=True)\n",
    "            totalEmbedding = self.calculateSimilarMatrix(totalSentence, getEmbedding=True)[0]\n",
    "            sentences      = list(zip(sentences, embedding))\n",
    "            embeddingShape = totalEmbedding.shape[0]\n",
    "            cluster        = UtilityFunction.cluster(sentences, countCluster)\n",
    "            totalSize      = []\n",
    "            countSize      = []\n",
    "            orderSentences = []\n",
    "            cluster.sort(key = lambda x: UtilityFunction.cosineSimilarity(x[0], totalEmbedding), reverse=True)\n",
    "            for clu in cluster:\n",
    "                clu[1].sort(key = lambda x: UtilityFunction.cosineSimilarity(x[1], clu[0]), reverse=True)\n",
    "                totalSize.append(len(clu[1]))\n",
    "                countSize.append(0)\n",
    "            totalSum = sum(totalSize)\n",
    "            count    = 0\n",
    "            index    = 0\n",
    "            while count < totalSum:\n",
    "                if countSize[index] < totalSize[index]:\n",
    "                    orderSentences.append(cluster[index][1][countSize[index]][0])\n",
    "                    countSize[index] += 1\n",
    "                    count += 1\n",
    "                index += 1\n",
    "                if index == countCluster:\n",
    "                    index = 0\n",
    "            sentences = orderSentences\n",
    "            \n",
    "        elif method == 'cosine':\n",
    "            docSentence    = [reduce(lambda x,y : x+y, doc) for doc in docList]\n",
    "            totalSentence  = reduce(lambda x,y : x+y, docSentence)\n",
    "            embedding      = self.calculateSimilarMatrix(sentences, getEmbedding=True)\n",
    "            totalEmbedding = self.calculateSimilarMatrix([totalSentence], getEmbedding=True)[0]\n",
    "            sentences      = list(zip(sentences, embedding))\n",
    "            sentences.sort(key = lambda x: UtilityFunction.cosineSimilarity(x[1], totalEmbedding), reverse=False)\n",
    "            sentences      = list(zip(*sentences))[0]\n",
    "            \n",
    "        elif method == 'none':\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            raise NameError(\" === Method '{}' Not Found == \".format(method))\n",
    "            \n",
    "        gc.collect()\n",
    "        return sentences\n",
    "    \n",
    "    \n",
    "    def getSummary(self, fileList, method='lexrank') -> str:\n",
    "        \"\"\"\n",
    "        Redundancy Control and Summary\n",
    "        \n",
    "        threshold: control similarity between sentences\n",
    "        size: summary length\n",
    "        \"\"\"\n",
    "        self.words = set()\n",
    "        docList    = []\n",
    "                                \n",
    "        for file in fileList:\n",
    "            docList.append(self.getFileText(file))\n",
    "            \n",
    "        for i in range(len(docList)):\n",
    "            doc         = docList[i].split('.')\n",
    "            docList[i]  = self.stemFunction(doc)\n",
    "            for sentence in docList[i]:\n",
    "                self.words |= set(sentence.split())\n",
    "        self.doc    = [reduce(lambda x,y : x+y, doc) for doc in docList]\n",
    "        self.docs   = reduce(lambda x,y : x+y, self.doc)\n",
    "        self.words -= self.stopwords\n",
    "        sentences   = self.sentencesSort(docList, method) \n",
    "        summary     = [sentences[0]]\n",
    "        \n",
    "        similarMatrix           = self.calculateSimilarMatrix(sentences)       \n",
    "        remainingSummarySize    = self.size - len(sentences[0])\n",
    "        lastSentencePosition    = 0\n",
    "        currentSentencePosition = 1\n",
    "        sentencesCount          = len(sentences) \n",
    "        while remainingSummarySize > 0 and currentSentencePosition < sentencesCount:\n",
    "            if similarMatrix[lastSentencePosition][currentSentencePosition] < self.simThreshold:\n",
    "                summary.append(copy.deepcopy(sentences[currentSentencePosition]\n",
    "                                             [: min(len(sentences[currentSentencePosition]), remainingSummarySize)]))\n",
    "                remainingSummarySize -= len(sentences[currentSentencePosition])\n",
    "                lastSentencePosition = currentSentencePosition\n",
    "            currentSentencePosition += 1\n",
    "        gc.collect()\n",
    "        return reduce(lambda x,y: x+y, summary)\n",
    "        \n",
    "  \n",
    "    def getBaselineSummary(self, fileList) -> str:\n",
    "        \"\"\"\n",
    "        Get first sentences in each document as baseline.\n",
    "        \"\"\"\n",
    "        baselineSummary = []\n",
    "        for file in fileList:\n",
    "            baselineSummary += [self.getFileText(file).split('.')[0]]\n",
    "        baselineSummary = UtilityFunction.stemFunction(baselineSummary)\n",
    "        return reduce(lambda x,y: x+y, baselineSummary)[:self.size]\n",
    "    \n",
    "    def testPerformance(self, performanceFunction, result, filelist, verbose=False) -> (float, float, float):\n",
    "        \"\"\"\n",
    "        Confusion Matrix Form\n",
    "        \n",
    "        True Positive |  False Positive\n",
    "        -------------------------------\n",
    "        True Negative |  False Negative\n",
    "        \n",
    "        Return Recall Precision and F-1 Score\n",
    "        \"\"\"\n",
    "        \n",
    "        total_recall    = 0\n",
    "        total_precision = 0\n",
    "        total_f1Score   = 0\n",
    "        \n",
    "        for i, filename in enumerate(filelist):\n",
    "            label = self.stemFunction([self.getLabelText(filename)])[0]\n",
    "            confusionMatrix = performanceFunction(result, label, self.doc[0])\n",
    "            recall    = confusionMatrix[0][0] / (confusionMatrix[0][0] + confusionMatrix[1][1])\n",
    "            precision = confusionMatrix[0][0] / (confusionMatrix[0][0] + confusionMatrix[0][1])\n",
    "            f1Score   = 2 * recall * precision / (recall + precision) \n",
    "            \n",
    "            total_recall    += recall\n",
    "            total_precision += precision\n",
    "            total_f1Score   += f1Score\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"    File {} - Recall : {:.4f} Precision : {:.4f} F1-Score : {:.4f}\".format(i, recall, precision, f1Score))\n",
    "        average_recall    = total_recall    / len(filelist)\n",
    "        average_precision = total_precision / len(filelist)\n",
    "        average_f1Score   = total_f1Score   / len(filelist)\n",
    "        \n",
    "        return average_recall, average_precision, average_f1Score\n",
    "    \n",
    "    def getLabelText(self, filename) -> str:\n",
    "        \"\"\"\n",
    "        Get All Content\n",
    "        \"\"\"\n",
    "        label = \"\"\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                label += line[:-1]\n",
    "        return label\n",
    "\n",
    "    \n",
    "    def getFileText(self, filename) -> str:\n",
    "        \"\"\"\n",
    "        Get content bewteen <TEXT> and </TEXT>\n",
    "        \"\"\"\n",
    "        with open(filename) as f:\n",
    "            doc = \"\"\n",
    "            addFlag = False\n",
    "            for line in f:\n",
    "                if line[:6] == '<TEXT>':\n",
    "                    addFlag = True\n",
    "                    continue\n",
    "                elif line[:7] == '</TEXT>':\n",
    "                    addFlag = False\n",
    "                if addFlag:\n",
    "                    doc += line[:-1]\n",
    "        return doc\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic Name :  d30001t\n",
      "Average Performance\n",
      "  Recall    : 0.3916\n",
      "  Precision : 0.3312\n",
      "  F1-Score  : 0.3589\n",
      "\n",
      "\n",
      "Topic Name :  d30002t\n",
      "Average Performance\n",
      "  Recall    : 0.2857\n",
      "  Precision : 0.2436\n",
      "  F1-Score  : 0.2630\n",
      "\n",
      "\n",
      "Topic Name :  d30003t\n",
      "Average Performance\n",
      "  Recall    : 0.4137\n",
      "  Precision : 0.3250\n",
      "  F1-Score  : 0.3640\n",
      "\n",
      "\n",
      "Topic Name :  d30005t\n",
      "Average Performance\n",
      "  Recall    : 0.3784\n",
      "  Precision : 0.3105\n",
      "  F1-Score  : 0.3410\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-c8889ea7733e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average Performance\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  Recall    : {:.4f}\\n  Precision : {:.4f}\\n  F1-Score  : {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    showSummary     = False\n",
    "    \n",
    "    # replace ROOT_PATH according to your folder position\n",
    "    ROOT_PATH       = '/home/willer/Desktop/Development/Python/dataset/Summary/'\n",
    "    XML_PATH        = ROOT_PATH + '04rouge.xml'\n",
    "    BASE_TOPIC_PATH = ROOT_PATH + 'DUC04/unpreprocess data/docs/'\n",
    "    BASE_LABEL_PATH = ROOT_PATH + 'DUC04/model/04model/'\n",
    "    \n",
    "    Pair = namedtuple('Pair', ['topicDirect', 'labelList'])\n",
    "    data = defaultdict(Pair)\n",
    "    \n",
    "    tree = ET.parse(xmlPath)\n",
    "    root = tree.getroot()\n",
    "    for element in root:\n",
    "        directoryName = element[3][0].text.split('.')[0]\n",
    "        labelList = []\n",
    "        for m in element[4]:\n",
    "            labelList += [m.text]\n",
    "        if directoryName not in data:\n",
    "            data[directoryName] = Pair(directoryName, labelList)\n",
    "    \n",
    "    for name, pair in data.items():\n",
    "        \n",
    "        print(\"\\n\\r\")\n",
    "        print(\"Topic Name : \", name)\n",
    "        topicDirectoryName, labelList = pair.topicDirect, pair.labelList\n",
    "        fileList  = []\n",
    "        labelList = [BASE_LABEL_PATH + file for file in labelList]\n",
    "        for file in os.listdir(BASE_PATH + topicDirectoryName):\n",
    "            if not file.startswith('._'):\n",
    "                fileList += [BASE_PATH + topicDirectoryName + '/' + file]\n",
    "        instance  = AutoSummary('stopwords.txt', UtilityFunction.stemFunction)\n",
    "        baseline  = instance.getBaselineSummary(fileList)\n",
    "        summary   = instance.getSummary(fileList, method='cosine')\n",
    "        r, p ,f   = instance.testPerformance(UtilityFunction.ROUGE_N, summary, labelList, verbose=False)\n",
    "        \n",
    "        if showSummary:\n",
    "            print(\"Summary:\\n\\r    \", summary)\n",
    "        print(\"Average Performance\")\n",
    "        print(\"  Recall    : {:.4f}\\n  Precision : {:.4f}\\n  F1-Score  : {:.4f}\".format(r, p, f))\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partially Test  \n",
    "\n",
    "| Method | Recall | Precision | F1-Score|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|BASELINE| 0.2475  | 0.3472    | 0.2890  |\n",
    "|cosine | 0.2178| 0.1897| 0.2027|\n",
    "|dochits| 0.3861| 0.3277| 0.3545|\n",
    "|lexrank| 0.3762| 0.3115| 0.3408|\n",
    "|cluster| 0.3960| 0.3225| 0.3557|\n",
    "|none   | 0.4257| 0.3644| 0.3926|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
