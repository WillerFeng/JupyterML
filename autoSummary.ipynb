{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import copy\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from collections import defaultdict, Counter\n",
    "from porterStemmer import PorterStemmer\n",
    "from functools import reduce\n",
    "from sklearn import cluster as skc\n",
    "warnings.filterwarnings('ignore')\n",
    "__author__ = 'willer'\n",
    "#             <--------------------------------->\n",
    "#             <                                 >\n",
    "#             <       SWIFT PHILOSOPHY !!       >\n",
    "#             <                                 >\n",
    "#             <--------------------------------->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtilityFunction:\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    @classmethod\n",
    "    def stemFunction(cls, sentences):\n",
    "        newSentences = []\n",
    "        for sentence in sentences:\n",
    "            output = ''\n",
    "            word = ''\n",
    "            for c in sentence:\n",
    "                if c.isalpha():\n",
    "                    word += c.lower()\n",
    "                else:\n",
    "                    if word:\n",
    "                        output += cls.stemmer.stem(word, 0, len(word)-1)\n",
    "                        word = ''\n",
    "                    output += c.lower()\n",
    "            if len(output) >= 5:\n",
    "                newSentences.append(output)\n",
    "        return newSentences\n",
    "    \n",
    "    @staticmethod\n",
    "    def ROUGE_N(result, label, total, n=1):\n",
    "        resultDictionary = Counter(result.split())\n",
    "        labelDictionary  = Counter(label.split())\n",
    "        totalDictionary  = Counter(total.split())\n",
    "        confusionMatrix = [[0.0, 0.0], \n",
    "                           [0.0, 0.0]]\n",
    "        for key, value in labelDictionary.items():\n",
    "            confusionMatrix[0][0] += min(value , resultDictionary[key])\n",
    "            confusionMatrix[1][1] += max(value - resultDictionary[key], 0)\n",
    "            \n",
    "        for key, value in resultDictionary.items():\n",
    "            confusionMatrix[0][1] += max(value - labelDictionary[key],0)\n",
    "            \n",
    "        labelDictionary += resultDictionary\n",
    "        for key, value in totalDictionary.items():\n",
    "            confusionMatrix[1][0] += value - labelDictionary[key]\n",
    "        \n",
    "        return confusionMatrix\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosineSimilarity(vectorA, vectorB):\n",
    "        return np.dot(vectorA, vectorB.transpose()) / (np.sqrt(np.sum(vectorA ** 2)) * np.sqrt(np.sum(vectorA ** 2)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def l2Distance(vectorA, vectorB):\n",
    "        return np.sqrt(np.sum((vectorA - vectorB) ** 2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def cluster(sentences, clusterSize):\n",
    "        X   = list(zip(*sentences))[1]\n",
    "        centroid, label, _ = skc.k_means(X , clusterSize)\n",
    "        res = []\n",
    "        for c in centroid:\n",
    "            res.append([c, []])\n",
    "        for i,l in enumerate(label):\n",
    "            res[l][1].append(sentences[i])\n",
    "        return res\n",
    "        \n",
    "    @staticmethod\n",
    "    def ROUGE_L():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "\r\n",
      "\r",
      " '' both ranariddh and sam rainsi have been outsid the countri sinc parliament wa ceremoni open on  hun sen's parti won 64 of the 122 seat in parliament in juli's nation elect, but not the two-third major necessari to form a govern on it  cite hun sen's threat to arrest opposit politician follow two alleg attempt on hi life, ranariddh and sam rainsi have said thei do not feel safe negoti insid the countri and ask the king to chair the summit at hi resid in  negoti to form the next govern have becom deadlock, and opposit parti leader princ norodom ranariddh and sam rainsi ar out of the countri follow threat of arrest from strongman hun '' the  govern and oppo\n",
      "\n",
      "\r\n",
      "Recall    : 0.376238\n",
      "Precision : 0.311475\n",
      "F1-Score  : 0.340807\n"
     ]
    }
   ],
   "source": [
    "class AutoSummary:\n",
    "    \n",
    "    def __init__(self, stopwordsFile, stemFunction):\n",
    "        \n",
    "        self.stemFunction = stemFunction\n",
    "        self.stopwords    = set()\n",
    "        self.words        = None\n",
    "        self.delta        = 0.00001\n",
    "        self.epsilon      = 0.0001\n",
    "        self.threshold    = 0.2\n",
    "        self.d            = 0.15\n",
    "        self.size         = 665\n",
    "        self.simThreshold = 0.7\n",
    "        with open(stopwordsFile) as f:\n",
    "            for line in f:\n",
    "                self.stopwords |= set(line.split())\n",
    "    \n",
    "    def calculateSimilarMatrix(self, sentences, docs=None, getEmbedding=False) -> [[float]]:\n",
    "        \"\"\"\n",
    "        sentences: [[str]]\n",
    "        Calculate sentences self-similar matrix or doc-sentence similar matrix\n",
    "        also embedding vector\n",
    "        \"\"\"\n",
    "        if docs:\n",
    "            tfSen  = pd.DataFrame(np.zeros((len(sentences),len(self.words))), columns=list(self.words))\n",
    "            tfDoc  = pd.DataFrame(np.zeros((len(docs)     ,len(self.words))), columns=list(self.words))\n",
    "            isfSen = pd.DataFrame(np.ones((1              ,len(self.words))), columns=list(self.words))\n",
    "            isfDoc = pd.DataFrame(np.ones((1              ,len(self.words))), columns=list(self.words))\n",
    "\n",
    "            for i,s in enumerate(sentences):\n",
    "                dic = Counter(s.split())\n",
    "                for k,v in dic.items():\n",
    "                    if k in self.words:\n",
    "                        tfSen .iloc[i][k] += v / len(s)\n",
    "                        isfSen.iloc[0][k] += 1\n",
    "                        \n",
    "            for i,d in enumerate(docs):\n",
    "                dic = Counter(d.split())\n",
    "                for k,v in dic.items():\n",
    "                    if k in self.words:\n",
    "                        tfDoc .iloc[i][k] += v / len(d)\n",
    "                        isfDoc.iloc[0][k] += 1\n",
    "                \n",
    "            isfSen = np.log(len(sentences) / isfSen) \n",
    "            isfDoc = np.log(len(docs)      / isfDoc)\n",
    "            for i in range(len(sentences)):\n",
    "                tfSen.iloc[i] = tfSen.iloc[i].mul(isfSen.iloc[0])\n",
    "            for i in range(len(docs)):\n",
    "                tfDoc.iloc[i] = tfDoc.iloc[i].mul(isfDoc.iloc[0])\n",
    "            \n",
    "            tfSen         = np.array(tfSen)\n",
    "            tfDoc         = np.array(tfDoc)\n",
    "            similarMatrix = np.dot(tfSen, tfDoc.transpose())\n",
    "            innerSen      = np.sum(np.multiply(tfSen, tfSen), axis=1, keepdims=True)\n",
    "            innerDoc      = np.sum(np.multiply(tfDoc, tfDoc), axis=1, keepdims=True)\n",
    "            product       = np.sqrt(np.dot(innerSen, innerDoc.transpose()))\n",
    "            similarMatrix = similarMatrix / product\n",
    "            \n",
    "        else:\n",
    "            tf  = pd.DataFrame(np.zeros((len(sentences),len(self.words))), columns=list(self.words))\n",
    "            isf = pd.DataFrame(np.ones((1,              len(self.words))), columns=list(self.words))\n",
    "\n",
    "            for i,s in enumerate(sentences):\n",
    "                dic = Counter(s.split())\n",
    "                for k,v in dic.items():\n",
    "                    if k in self.words:\n",
    "                        tf .iloc[i][k] += v / len(s)\n",
    "                        isf.iloc[0][k] += 1\n",
    "                        \n",
    "            isf = np.log(len(sentences) / isf) \n",
    "            for i in range(len(sentences)):\n",
    "                tf.iloc[i] = tf.iloc[i].mul(isf.iloc[0])\n",
    "            tf_isf_Matrix  = np.array(tf)\n",
    "            if getEmbedding:\n",
    "                return tf_isf_Matrix\n",
    "            innerMatrix    = np.sum(np.multiply(tf_isf_Matrix, tf_isf_Matrix), axis=1, keepdims=True)\n",
    "            innerMatrix    = np.sqrt(np.dot(innerMatrix, innerMatrix.transpose()))\n",
    "            similarMatrix  = np.dot(tf_isf_Matrix, tf_isf_Matrix.transpose())\n",
    "            similarMatrix /= innerMatrix\n",
    "        \n",
    "        gc.collect()\n",
    "        return similarMatrix\n",
    "    \n",
    "    \n",
    "    def sentencesSort(self, docList, method) -> [str]:\n",
    "        \"\"\"\n",
    "        docList = [[str]]\n",
    "        method in {'lexrank' , 'dochits', 'cluster', 'cosine', 'none'}\n",
    "        \"\"\"\n",
    "        sentences = reduce(lambda x,y : x+y, docList)\n",
    "        \n",
    "        \n",
    "        if method == 'lexrank':\n",
    "\n",
    "            similarMatrix = self.calculateSimilarMatrix(sentences)\n",
    "            degree        = np.zeros(len(sentences))\n",
    "            for i in range(len(sentences)):\n",
    "                for j in range(len(sentences)):\n",
    "                    if similarMatrix[i][j] > self.threshold:\n",
    "                        similarMatrix[i][j] = 1\n",
    "                        degree[i]          += 1\n",
    "                    else:\n",
    "                        similarMatrix[i][j] = 0\n",
    "                \n",
    "            for i in range(len(sentences)):\n",
    "                for j in range(len(sentences)):\n",
    "                    similarMatrix[i][j]    /= degree[i]\n",
    "                \n",
    "            U = np.ones((len(sentences), len(sentences))) / len(sentences)\n",
    "            similarMatrix = self.d * U + (1 - self.d) * similarMatrix\n",
    "            last_p = p = np.ones(len(sentences)) / len(sentences)\n",
    "            while True:\n",
    "                p    = np.dot(similarMatrix.transpose(), p)\n",
    "                loss = np.sum(np.abs(p - last_p))\n",
    "                if loss < self.epsilon:\n",
    "                    break\n",
    "                last_p = p     \n",
    "            sentences = list(zip(sentences, p))\n",
    "            sentences.sort(key = lambda x: x[1], reverse=True)\n",
    "            sentences = list(zip(*sentences))[0]\n",
    "        \n",
    "        elif method == 'dochits':\n",
    "    \n",
    "            docs   = [reduce(lambda x,y : x+y, doc) for doc in docList]\n",
    "            L      = self.calculateSimilarMatrix(sentences, docs)\n",
    "            last_A = A = np.ones((len(sentences), 1))\n",
    "            last_H = H = np.ones((len(docs)     , 1))\n",
    "            while True:\n",
    "                A  = np.dot(L            , last_H)\n",
    "                H  = np.dot(L.transpose(), last_A)\n",
    "                A /= np.linalg.norm(A)\n",
    "                H /= np.linalg.norm(H)\n",
    "                loss_A = np.sum(last_A - A)\n",
    "                loss_H = np.sum(last_H - H)\n",
    "                if max(loss_A, loss_H) < self.delta:\n",
    "                    break\n",
    "                last_A = A\n",
    "                last_H = H\n",
    "            sentences = list(zip(sentences, A))\n",
    "            sentences.sort(key = lambda x: x[1], reverse=True)\n",
    "            sentences = list(zip(*sentences))[0]\n",
    "            \n",
    "        elif method == 'cluster':\n",
    "            docSentence    = [reduce(lambda x,y : x+y, doc) for doc in docList]\n",
    "            totalSentence  = reduce(lambda x,y : x+y, docSentence)\n",
    "            countCluster   = math.floor(np.sqrt(len(sentences)))\n",
    "            embedding      = self.calculateSimilarMatrix(sentences, getEmbedding=True)\n",
    "            totalEmbedding = self.calculateSimilarMatrix(totalSentence, getEmbedding=True)[0]\n",
    "            sentences      = list(zip(sentences, embedding))\n",
    "            embeddingShape = totalEmbedding.shape[0]\n",
    "            cluster        = UtilityFunction.cluster(sentences, countCluster)\n",
    "            totalSize      = []\n",
    "            countSize      = []\n",
    "            orderSentences = []\n",
    "            cluster.sort(key = lambda x: UtilityFunction.cosineSimilarity(x[0], totalEmbedding), reverse=True)\n",
    "            for clu in cluster:\n",
    "                clu[1].sort(key = lambda x: UtilityFunction.cosineSimilarity(x[1], clu[0]), reverse=True)\n",
    "                totalSize.append(len(clu[1]))\n",
    "                countSize.append(0)\n",
    "            totalSum = sum(totalSize)\n",
    "            count    = 0\n",
    "            index    = 0\n",
    "            while count < totalSum:\n",
    "                if countSize[index] < totalSize[index]:\n",
    "                    orderSentences.append(cluster[index][1][countSize[index]][0])\n",
    "                    countSize[index] += 1\n",
    "                    count += 1\n",
    "                index += 1\n",
    "                if index == countCluster:\n",
    "                    index = 0\n",
    "            sentences = orderSentences\n",
    "            \n",
    "        elif method == 'cosine':\n",
    "            docSentence    = [reduce(lambda x,y : x+y, doc) for doc in docList]\n",
    "            totalSentence  = reduce(lambda x,y : x+y, docSentence)\n",
    "            embedding      = self.calculateSimilarMatrix(sentences, getEmbedding=True)\n",
    "            totalEmbedding = self.calculateSimilarMatrix([totalSentence], getEmbedding=True)[0]\n",
    "            sentences      = list(zip(sentences, embedding))\n",
    "            sentences.sort(key = lambda x: UtilityFunction.cosineSimilarity(x[1], totalEmbedding), reverse=False)\n",
    "            sentences      = list(zip(*sentences))[0]\n",
    "            \n",
    "        elif method == 'none':\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            raise NameError(\" === Method '{}' Not Found == \".format(method))\n",
    "            \n",
    "        gc.collect()\n",
    "        return sentences\n",
    "    \n",
    "    \n",
    "    def getSummary(self, fileList, method='lexrank') -> str:\n",
    "        \"\"\"\n",
    "        Redundancy Control and Summary\n",
    "        \n",
    "        threshold: control similarity between sentences\n",
    "        size: summary length\n",
    "        \"\"\"\n",
    "        self.words = set()\n",
    "        docList    = []\n",
    "                                \n",
    "        for file in fileList:\n",
    "            docList.append(self.getFileText(file))\n",
    "            \n",
    "        for i in range(len(docList)):\n",
    "            doc         = docList[i].split('.')\n",
    "            docList[i]  = self.stemFunction(doc)\n",
    "            for sentence in docList[i]:\n",
    "                self.words |= set(sentence.split())\n",
    "        self.doc    = [reduce(lambda x,y : x+y, doc) for doc in docList]\n",
    "        self.docs   = reduce(lambda x,y : x+y, self.doc)\n",
    "        self.words -= self.stopwords\n",
    "        sentences   = self.sentencesSort(docList, method) \n",
    "        summary     = [sentences[0]]\n",
    "        similarMatrix           = self.calculateSimilarMatrix(sentences)       \n",
    "        remainingSummarySize    = self.size - len(sentences[0])\n",
    "        lastSentencePosition    = 0\n",
    "        currentSentencePosition = 1\n",
    "        sentencesCount          = len(sentences) \n",
    "        while remainingSummarySize > 0 and currentSentencePosition < sentencesCount:\n",
    "            if similarMatrix[lastSentencePosition][currentSentencePosition] < self.simThreshold:\n",
    "                summary.append(copy.deepcopy(sentences[currentSentencePosition]\n",
    "                                             [: min(len(sentences[currentSentencePosition]), remainingSummarySize)]))\n",
    "                remainingSummarySize -= len(sentences[currentSentencePosition])\n",
    "                lastSentencePosition = currentSentencePosition\n",
    "            currentSentencePosition += 1\n",
    "        gc.collect()\n",
    "        return reduce(lambda x,y: x+y, summary)\n",
    "        \n",
    "  \n",
    "    def getBaselineSummary(self, fileList) -> str:\n",
    "        \"\"\"\n",
    "        Get first sentences in each document as baseline.\n",
    "        \"\"\"\n",
    "        baselineSummary = []\n",
    "        for file in fileList:\n",
    "            baselineSummary += [self.getFileText(file).split('.')[0]]\n",
    "        baselineSummary = UtilityFunction.stemFunction(baselineSummary)\n",
    "        return reduce(lambda x,y: x+y, baselineSummary)[:self.size]\n",
    "    \n",
    "    def testPerformance(self, performanceFunction, result, filename) -> (float, float, float):\n",
    "        \"\"\"\n",
    "        Confusion Matrix Form\n",
    "        \n",
    "        True Positive |  False Positive\n",
    "        -------------------------------\n",
    "        True Negative |  False Negative\n",
    "        \n",
    "        Return Recall Precision and F-1 Score\n",
    "        \"\"\"\n",
    "        \n",
    "        label = self.stemFunction([self.getLabelText(filename)])[0]\n",
    "        confusionMatrix = performanceFunction(result, label, self.doc[0])\n",
    "        recall    = confusionMatrix[0][0] / (confusionMatrix[0][0] + confusionMatrix[1][1])\n",
    "        precision = confusionMatrix[0][0] / (confusionMatrix[0][0] + confusionMatrix[0][1])\n",
    "        f1Score   = 2 * recall * precision / (recall + precision) \n",
    "        \n",
    "        return recall, precision, f1Score\n",
    "    \n",
    "    def getLabelText(self, filename) -> str:\n",
    "        \"\"\"\n",
    "        Get All Content\n",
    "        \"\"\"\n",
    "        label = \"\"\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                label += line[:-1]\n",
    "        return label\n",
    "\n",
    "    \n",
    "    def getFileText(self, filename) -> str:\n",
    "        \"\"\"\n",
    "        Get content bewteen <TEXT> and </TEXT>\n",
    "        \"\"\"\n",
    "        with open(filename) as f:\n",
    "            doc = \"\"\n",
    "            addFlag = False\n",
    "            for line in f:\n",
    "                if line[:6] == '<TEXT>':\n",
    "                    addFlag = True\n",
    "                    continue\n",
    "                elif line[:7] == '</TEXT>':\n",
    "                    addFlag = False\n",
    "                if addFlag:\n",
    "                    doc += line[:-1]\n",
    "        return doc\n",
    "           \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    PATH      = '/Users/willer/Downloads/dataset/DUC04/unpreprocess data/docs/'\n",
    "    PATH_RES  = '/Users/willer/Downloads/dataset/DUC04/model/04model/'\n",
    "    fileL     = ['d30001t/APW19981016.0240', 'd30001t/APW19981022.0269', 'd30001t/APW19981026.0220']\n",
    "    \n",
    "    # 将一整个topic的十个文件加载到fileList里面\n",
    "    fileList  = [PATH + f for f in fileL]\n",
    "    # 对应topic的专家摘要\n",
    "    labelfile = PATH_RES + 'D30001.M.100.T.A'\n",
    "    \n",
    "    \n",
    "    instance  = AutoSummary('stopwords.txt', UtilityFunction.stemFunction)\n",
    "    baseline  = instance.getBaselineSummary(fileList)\n",
    "    summary   = instance.getSummary(fileList, method='lexrank')\n",
    "    r, p ,f   = instance.testPerformance(UtilityFunction.ROUGE_N, summary, labelfile)\n",
    "    print(\"Summary:\\n\\r\\n\\r\", summary)\n",
    "    print(\"\\n\\r\")\n",
    "    print(\"Recall    : {:.6f}\\nPrecision : {:.6f}\\nF1-Score  : {:.6f}\".format(r, p, f))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | Recall | Precision | F1-Score|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|BASELINE| 0.2475  | 0.3472    | 0.2890  |\n",
    "|cosine | 0.2178| 0.1897| 0.2027|\n",
    "|dochits| 0.3861| 0.3277| 0.3545|\n",
    "|lexrank| 0.3762| 0.3115| 0.3408|\n",
    "|cluster| 0.3960| 0.3225| 0.3557|\n",
    "|none   | 0.4257| 0.3644| 0.3926|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda11cf69b2120d45aca851a42bf4b8015b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
