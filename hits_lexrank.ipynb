{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import warnings\n",
    "from collections import defaultdict, Counter\n",
    "import docx\n",
    "import copy\n",
    "from docx import Document\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, \n",
      "    loss_A = 7.871425985689779\n",
      "    loss_H = 2.9145164810236213\n",
      "====================\n",
      "epoch : 2, \n",
      "    loss_A = 0.007199111899645255\n",
      "    loss_H = -0.11883170218737157\n",
      "====================\n",
      "epoch : 3, \n",
      "    loss_A = -0.05118293143918745\n",
      "    loss_H = 0.33746858097169685\n",
      "====================\n",
      "epoch : 4, \n",
      "    loss_A = 0.2943989381362246\n",
      "    loss_H = -0.2126701282555522\n",
      "====================\n",
      "epoch : 5, \n",
      "    loss_A = -0.19842859905944235\n",
      "    loss_H = 0.5061958861500001\n",
      "====================\n",
      "epoch : 6, \n",
      "    loss_A = 0.520863015722979\n",
      "    loss_H = -0.2659037639357232\n",
      "====================\n",
      "epoch : 7, \n",
      "    loss_A = -0.2679565909646284\n",
      "    loss_H = 0.5461394333274807\n",
      "====================\n",
      "epoch : 8, \n",
      "    loss_A = 0.5639813849721385\n",
      "    loss_H = -0.24478964403735898\n",
      "====================\n",
      "epoch : 9, \n",
      "    loss_A = -0.24531494490660258\n",
      "    loss_H = 0.4617772146857329\n",
      "====================\n",
      "epoch : 10, \n",
      "    loss_A = 0.4696489219942167\n",
      "    loss_H = -0.18576796257186806\n",
      "====================\n",
      "epoch : 11, \n",
      "    loss_A = -0.18537798455271493\n",
      "    loss_H = 0.33639640229155243\n",
      "====================\n",
      "epoch : 12, \n",
      "    loss_A = 0.34006574004626333\n",
      "    loss_H = -0.12754571812436727\n",
      "====================\n",
      "epoch : 13, \n",
      "    loss_A = -0.12771607186427353\n",
      "    loss_H = 0.22682729755121764\n",
      "====================\n",
      "epoch : 14, \n",
      "    loss_A = 0.2296512355494348\n",
      "    loss_H = -0.08354215231314288\n",
      "====================\n",
      "epoch : 15, \n",
      "    loss_A = -0.0841828306623616\n",
      "    loss_H = 0.1474267834581045\n",
      "====================\n",
      "epoch : 16, \n",
      "    loss_A = 0.149881750415331\n",
      "    loss_H = -0.053549796030438856\n",
      "====================\n",
      "epoch : 17, \n",
      "    loss_A = -0.05429318115216211\n",
      "    loss_H = 0.09419268506965786\n",
      "====================\n",
      "epoch : 18, \n",
      "    loss_A = 0.09616118103372287\n",
      "    loss_H = -0.0339807153547243\n",
      "====================\n",
      "epoch : 19, \n",
      "    loss_A = -0.03462490092879869\n",
      "    loss_H = 0.059692303513706774\n",
      "====================\n",
      "epoch : 20, \n",
      "    loss_A = 0.061145507783282094\n",
      "    loss_H = -0.021458195823812865\n",
      "====================\n",
      "epoch : 21, \n",
      "    loss_A = -0.021947443074539497\n",
      "    loss_H = 0.03767624755321336\n",
      "====================\n",
      "epoch : 22, \n",
      "    loss_A = 0.03869002255910314\n",
      "    loss_H = -0.013517321036684626\n",
      "====================\n",
      "epoch : 23, \n",
      "    loss_A = -0.013863556209257721\n",
      "    loss_H = 0.02373067810047963\n",
      "====================\n",
      "epoch : 24, \n",
      "    loss_A = 0.024412702663130198\n",
      "    loss_H = -0.008504190565788133\n",
      "====================\n",
      "epoch : 25, \n",
      "    loss_A = -0.00873932113741338\n",
      "    loss_H = 0.014930126365080926\n",
      "====================\n",
      "epoch : 26, \n",
      "    loss_A = 0.015378474814633891\n",
      "    loss_H = -0.005346595667898163\n",
      "====================\n",
      "epoch : 27, \n",
      "    loss_A = -0.005502269112384661\n",
      "    loss_H = 0.009387374097420076\n",
      "====================\n",
      "epoch : 28, \n",
      "    loss_A = 0.00967775771889648\n",
      "    loss_H = -0.0033601544508519987\n",
      "====================\n",
      "epoch : 29, \n",
      "    loss_A = -0.0034615583894400948\n",
      "    loss_H = 0.0059002347583932135\n",
      "====================\n",
      "epoch : 30, \n",
      "    loss_A = 0.006086494699863693\n",
      "    loss_H = -0.0021113125093588925\n",
      "====================\n",
      "epoch : 31, \n",
      "    loss_A = -0.0021766607019514106\n",
      "    loss_H = 0.0037077011878888233\n",
      "====================\n",
      "epoch : 32, \n",
      "    loss_A = 0.0038264105422678996\n",
      "    loss_H = -0.0013264726343947542\n",
      "====================\n",
      "epoch : 33, \n",
      "    loss_A = -0.0013682807654828433\n",
      "    loss_H = 0.002329636921348577\n",
      "====================\n",
      "epoch : 34, \n",
      "    loss_A = 0.002404969943505808\n",
      "    loss_H = -0.0008333350900421865\n",
      "====================\n",
      "epoch : 35, \n",
      "    loss_A = -0.0008599492532519182\n",
      "    loss_H = 0.001463665785247703\n",
      "====================\n",
      "epoch : 36, \n",
      "    loss_A = 0.0015113327696431034\n",
      "    loss_H = -0.0005235155581763407\n",
      "====================\n",
      "epoch : 37, \n",
      "    loss_A = -0.0005403980607461556\n",
      "    loss_H = 0.0009195572124718054\n",
      "====================\n",
      "epoch : 38, \n",
      "    loss_A = 0.0009496578483664808\n",
      "    loss_H = -0.00032887828144981257\n",
      "====================\n",
      "epoch : 39, \n",
      "    loss_A = -0.00033956072772309276\n",
      "    loss_H = 0.0005777053313287817\n",
      "====================\n",
      "epoch : 40, \n",
      "    loss_A = 0.0005966864943936223\n",
      "    loss_H = -0.00020660472837566446\n",
      "====================\n",
      "epoch : 41, \n",
      "    loss_A = -0.00021335181804704728\n",
      "    loss_H = 0.00036293531599199125\n",
      "====================\n",
      "epoch : 42, \n",
      "    loss_A = 0.0003748927430152482\n",
      "    loss_H = -0.00012979161847126441\n",
      "====================\n",
      "epoch : 43, \n",
      "    loss_A = -0.00013404746393494427\n",
      "    loss_H = 0.0002280078863654833\n",
      "====================\n",
      "epoch : 44, \n",
      "    loss_A = 0.0002355352543265965\n",
      "    loss_H = -8.153708123415093e-05\n",
      "====================\n",
      "epoch : 45, \n",
      "    loss_A = -8.421890033599722e-05\n",
      "    loss_H = 0.00014324178140592836\n",
      "====================\n",
      "epoch : 46, \n",
      "    loss_A = 0.00014797792067469673\n",
      "    loss_H = -5.1223117004256524e-05\n",
      "====================\n",
      "Authoritier Vector : \n",
      "[[-0.66549751  0.12929414  0.18656419  0.21235177  0.24882044  0.36864605\n",
      "   0.18318146  0.18897132  0.18013447  0.2914874   0.27569215]]\n",
      "Hub Vector : \n",
      "[[-0.70632347  0.30189534  0.44453666  0.19433576  0.41783625]]\n"
     ]
    }
   ],
   "source": [
    "replace_list = [',', '(', ')', '\\\"', '.', '?', ':', ';', '“', '”']\n",
    "class DocHITS():\n",
    "    def norm(self, matrix):\n",
    "        return matrix / np.linalg.norm(matrix)\n",
    "    \n",
    "    def cal_L(self, sentences, docs, belong):\n",
    "        columns = set()\n",
    "        for i in range(len(sentences)):\n",
    "            for rep in replace_list:\n",
    "                sentences[i] = sentences[i].replace(rep, ' ')\n",
    "            sentences[i] = sentences[i].lower()\n",
    "            sentences[i] = sentences[i].split()\n",
    "            columns |= set(sentences[i])\n",
    "        \n",
    "        doc_length = [0] * len(docs)\n",
    "        for i, s in enumerate(sentences):\n",
    "            doc_length[belong[i]] += len(s)\n",
    "        \n",
    "        tf_sen = np.zeros((len(sentences),len(columns)))\n",
    "        tf_s = pd.DataFrame(tf_sen, columns=list(columns))\n",
    "        tf_doc = np.zeros((len(docs),len(columns)))\n",
    "        tf_d = pd.DataFrame(tf_doc, columns=list(columns))\n",
    "        \n",
    "        isf_np1 = np.ones((1, len(columns)))\n",
    "        isf_np2 = np.ones((1, len(columns)))\n",
    "        isf_sen = pd.DataFrame(isf_np1, columns=list(columns))\n",
    "        isf_doc = pd.DataFrame(isf_np2, columns=list(columns))\n",
    "        \n",
    "        for i,s in enumerate(sentences):\n",
    "            dic = Counter(s)\n",
    "            for k,v in dic.items():\n",
    "                tf_s.iloc[i][k] += v / len(s)\n",
    "                tf_d.iloc[belong[i]][k] += v / doc_length[belong[i]]\n",
    "                isf_sen.iloc[0][k] += 1\n",
    "                isf_doc.iloc[0][k] += 1\n",
    "                \n",
    "        for c in columns:\n",
    "            isf_sen.iloc[0][c] = np.log(len(sentences) / (isf_sen.iloc[0][c] + 1))\n",
    "            isf_doc.iloc[0][c] = np.log(len(docs) / (isf_doc.iloc[0][c] + 1))\n",
    "            \n",
    "        for i in range(len(sentences)):\n",
    "            tf_s.iloc[i] = tf_s.iloc[i].mul(isf_sen.iloc[0])\n",
    "        for i in range(len(docs)):\n",
    "            tf_d.iloc[i] = tf_d.iloc[i].mul(isf_doc.iloc[0])\n",
    "            \n",
    "        tf_sen = np.array(tf_s)\n",
    "        tf_doc = np.array(tf_d)\n",
    "        similar =np.dot(tf_sen, tf_doc.transpose())\n",
    "        inner_sen = np.sum(np.multiply(tf_sen, tf_sen), axis=1, keepdims=True)\n",
    "        inner_doc = np.sum(np.multiply(tf_doc, tf_doc), axis=1, keepdims=True)\n",
    "        inner = np.sqrt(np.dot(inner_sen, inner_doc.transpose()))\n",
    "        self.L = similar / inner\n",
    "            \n",
    "    def run(self, shape_A, shape_H, delta):\n",
    "        epoch = 1\n",
    "        last_A = A = np.ones((shape_A, 1))\n",
    "        last_H = H = np.ones((shape_H, 1))\n",
    "        while True:\n",
    "            A = self.norm(np.dot(self.L, last_H))\n",
    "            H = self.norm(np.dot(self.L.transpose(), last_A))\n",
    "            loss_A = np.sum(last_A - A)\n",
    "            loss_H = np.sum(last_H - H)\n",
    "            if max(loss_A, loss_H) < delta:\n",
    "                return A.transpose(), H.transpose()\n",
    "            print(\"epoch : {}, \\n    loss_A = {}\\n    loss_H = {}\".format(epoch, loss_A, loss_H))\n",
    "            print('='*20)\n",
    "            epoch += 1\n",
    "            last_A = A\n",
    "            last_H = H\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    path = '/Users/willer/Desktop/data.docx'\n",
    "    document = Document(path)\n",
    "    table = document.tables\n",
    "    table = table[0]\n",
    "    sentences = []\n",
    "    docs = [\"\"]*5\n",
    "    belong = {}\n",
    "    for i,line in enumerate(table.rows[1:]):\n",
    "        docs[int((line.cells[1].text)[1])-1] += line.cells[-1].text + \" \"\n",
    "        belong[i] = int((line.cells[1].text)[1])-1\n",
    "        sentences.append(line.cells[-1].text)\n",
    "        \n",
    "    doc_model = DocHITS()\n",
    "    doc_model.cal_L(sentences, docs, belong)\n",
    "    vec_A, vec_H = doc_model.run(len(sentences), len(docs), 0.0001)\n",
    "    \n",
    "    print(\"Authoritier Vector : \\n{}\".format(vec_A))\n",
    "    print(\"Hub Vector : \\n{}\".format(vec_H))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-095ca8b15900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/willer/Desktop/data.docx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Document' is not defined"
     ]
    }
   ],
   "source": [
    "class LexRank():\n",
    "    \n",
    "    def cal_lexrank(self, sentences, threshold):\n",
    "        columns = set()\n",
    "        for i in range(len(sentences)):\n",
    "            for rep in replace_list:\n",
    "                sentences[i] = sentences[i].replace(rep, ' ')\n",
    "            sentences[i] = sentences[i].lower()\n",
    "            sentences[i] = sentences[i].split()\n",
    "            columns |= set(sentences[i])\n",
    "\n",
    "        tf_np = np.zeros((len(sentences),len(columns)))\n",
    "        tf = pd.DataFrame(tf_np, columns=list(columns))\n",
    "        isf_np = np.ones((1, len(columns)))\n",
    "        isf = pd.DataFrame(isf_np, columns=list(columns))\n",
    "\n",
    "\n",
    "        for i,s in enumerate(sentences):\n",
    "            dic = Counter(s)\n",
    "            for k,v in dic.items():\n",
    "                tf .iloc[i][k] += v / len(s)\n",
    "                isf.iloc[0][k] += 1\n",
    "                    \n",
    "        isf = np.log(len(sentences) / isf)\n",
    "        for i in range(len(sentences)):\n",
    "            tf.iloc[i] = tf.iloc[i].mul(isf.iloc[0])\n",
    "        \n",
    "        tf_isf_matrix = np.array(tf)\n",
    "        similar_matrix = np.dot(tf_isf_matrix, tf_isf_matrix.transpose())\n",
    "        inner_matrix = np.sum(np.multiply(tf_isf_matrix, tf_isf_matrix), axis=1, keepdims=True)\n",
    "        inner_matrix = np.sqrt(np.dot(inner_matrix, inner_matrix.transpose()))\n",
    "        similar_matrix /= inner_matrix\n",
    "        \n",
    "        degree = np.zeros(len(sentences))\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences)):\n",
    "                if similar_matrix[i][j] > threshold:\n",
    "                    similar_matrix[i][j] = 1\n",
    "                    degree[i] += 1\n",
    "                else:\n",
    "                    similar_matrix[i][j] = 0\n",
    "           \n",
    "        self.similar = copy.deepcopy(similar_matrix)\n",
    "        for i in range(len(sentences)):\n",
    "            self.similar[i] /= np.sum(self.similar[i])\n",
    "        U = np.ones((len(sentences), len(sentences)))\n",
    "        U /= len(sentences)\n",
    "        self.similar = d * U + (1 - d) * self.similar\n",
    "\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences)):\n",
    "                similar_matrix[i][j] /= degree[i]\n",
    "                \n",
    "        return self.power_method(similar_matrix, len(sentences), 0.0001)\n",
    "        \n",
    "    def power_method(self, matrix, N, epsilon):\n",
    "        last_p = p = np.ones(N) / N\n",
    "        loss = float('inf')\n",
    "        epoch = 1\n",
    "        while True:\n",
    "            p = np.dot(self.similar.transpose(), p)\n",
    "            loss = np.sum(np.abs(p - last_p))\n",
    "            if loss < epsilon:\n",
    "                return p\n",
    "            print(\"epoch {}\\n    loss:{}\".format(epoch, loss))\n",
    "            print(\"=\"*20)\n",
    "            last_p = p\n",
    "            epoch += 1\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    path = '/Users/willer/Desktop/data.docx'\n",
    "    document = Document(path)\n",
    "    table = document.tables\n",
    "    table = table[0]\n",
    "    sentences = []\n",
    "    for line in table.rows[1:]:\n",
    "        sentences.append(line.cells[-1].text)\n",
    "        \n",
    "    lex_model = LexRank()\n",
    "    Lex_mat = lex_model.cal_lexrank(sentences, 0.2)\n",
    "    print(Lex_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda11cf69b2120d45aca851a42bf4b8015b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
